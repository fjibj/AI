{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lIYdn1woOS1n"},"outputs":[],"source":["!python -V"]},{"cell_type":"code","source":["!python -m pip install cpm_kernels openai chromadb langchain pypdf unstructured pinecone-client"],"metadata":{"id":"9gfYwNuLSKsc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sentence_transformers"],"metadata":{"id":"Q1ZR54AHS2xj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#上传文档到pinecone，需预先注册好pinecone账号，并创建一个index\n","import sys\n","print(sys.executable)\n","\n","import site\n","print(site.getsitepackages())\n","\n","from langchain.chat_models import ChatOpenAI\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import Chroma, Pinecone\n","from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n","from langchain.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader, UnstructuredWordDocumentLoader\n","from langchain.chains.retrieval_qa.base import RetrievalQA\n","\n","# 输入你的repo\n","#file_path = './YOURREPO/' \n","file_path = 'xxxxxxx'  # 填写你文档存放目录的绝对路径\n","\n","# 根据文件类型来定义一个loader，不同的loader能够解析不同的文件内容，最终都会解析为一个大文本\n","#loader = DirectoryLoader(file_path, glob=\"*.docx\", loader_cls=UnstructuredWordDocumentLoader)\n","# 类似方法加载其他文档类型\n","loader = DirectoryLoader(file_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n","\n","documents = loader.load()\n","print(len(documents))\n","\n","\n","# 定义文本分块的规则，这里用了一个很简单的规则，按照默认的分隔符来切割文本，使得每一段一定数量字符\n","doc_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n","                                              chunk_overlap=200,\n","                                              length_function = len)\n","chunks = doc_splitter.split_documents(documents)\n","print(len(chunks))\n","\n","\n","# 定义文本的embedding，也就是如何把文本转换为向量。默认使用sentence-transformers这个免费的模型，也可以使用OpenAI提供的收费接口\n","# embeddings = OpenAIEmbeddings()\n","# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n","\n","#embeddings = HuggingFaceEmbeddings(model_name=\"GanymedeNil/text2vec-large-chinese\")\n","embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n","\n","\n","\n","import pinecone\n","\n","# initialize pinecone\n","\n","pinecone.init(\n","    api_key=\"xxxxxxxx\",  # 填写你自己的API key， find at app.pinecone.io\n","    environment=\"xxxxxxxxx\"  # 填写environment地址 next to api key in console\n",")\n","\n","index_name = \"xxxxxxx\"  #填写你自己的index名字\n","\n","#清空pinecone索引\n","index = pinecone.Index(index_name) \n","# delete_response = index.delete(deleteAll=True)\n","# print(delete_response)\n","\n","\n","db = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n","\n","index_stats_response = index.describe_index_stats()\n","# 最后打印一下status看看是不是都存进去了\n","print(index_stats_response)"],"metadata":{"id":"A22iSyLYTTnx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gradio openai"],"metadata":{"id":"ndIxBwI4cWkb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#打开知识库对话界面\n","#注：在弹出页面的Database[Tab页]中填入pinecone key,Pinecone Environment和Pinecone Index，\n","#在OpenAI API Key中填入自己的API之后点击Initialize Model按钮，初始化后即可开始对话\n","import gradio as gr\n","import random\n","import time\n","\n","from langchain import PromptTemplate\n","from langchain.llms import OpenAI\n","from langchain.chat_models import ChatOpenAI\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import Pinecone\n","from langchain.chains import LLMChain\n","from langchain.chains.retrieval_qa.base import RetrievalQA\n","from langchain.chains.question_answering import load_qa_chain\n","import pinecone\n","\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","#OPENAI_API_KEY = \"\"\n","OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n","OPENAI_TEMP  = 1\n","OPENAI_API_LINK = \"[OpenAI API Key](https://platform.openai.com/account/api-keys)\"\n","OPENAI_LINK = \"[OpenAI](https://openai.com)\"\n","\n","PINECONE_KEY = os.environ.get(\"PINECONE_KEY\", \"\")\n","PINECONE_ENV = os.environ.get(\"PINECONE_ENV\", \"\")\n","PINECONE_INDEX = os.environ.get(\"PINECONE_INDEX\", '')\n","\n","PINECONE_LINK  = \"[Pinecone](https://www.pinecone.io)\"\n","LANGCHAIN_LINK = \"[LangChain](https://python.langchain.com/en/latest/index.html)\"\n","\n","EMBEDDING_MODEL = os.environ.get(\"PINECONE_INDEX\", \"sentence-transformers/all-mpnet-base-v2\")\n","\n","# return top-k text chunks from vector store\n","TOP_K_DEFAULT = 15\n","TOP_K_MAX = 30\n","\n","\n","BUTTON_MIN_WIDTH = 215\n","\n","LLM_NULL = \"LLM-UNLOAD-critical\"\n","LLM_DONE = \"LLM-LOADED-9cf\"\n","\n","DB_NULL = \"DB-UNLOAD-critical\"\n","DB_DONE = \"DB-LOADED-9cf\"\n","\n","FORK_BADGE = \"Fork-HuggingFace Space-9cf\"\n","\n","\n","def get_logo(inputs, logo) -> str:\n","    return f\"\"\"https://img.shields.io/badge/{inputs}?style=flat&logo={logo}&logoColor=white\"\"\"\n","\n","def get_status(inputs, logo, pos) -> str:\n","    return f\"\"\"<img\n","    src   = \"{get_logo(inputs, logo)}\";\n","    style = \"margin: 0 auto; float:{pos}\"\n","    >\"\"\"\n","    \n","\n","KEY_INIT   = \"Initialize Model\"\n","KEY_SUBMIT = \"Submit\"\n","KEY_CLEAR  = \"Clear\"\n","\n","MODEL_NULL = get_status(LLM_NULL, \"openai\", \"left\")\n","MODEL_DONE = get_status(LLM_DONE, \"openai\", \"left\")\n","\n","DOCS_NULL = get_status(DB_NULL, \"processingfoundation\", \"right\")\n","DOCS_DONE = get_status(DB_DONE, \"processingfoundation\", \"right\")\n","\n","TAB_1 = \"Chatbot\"\n","TAB_2 = \"Details\"\n","TAB_3 = \"Database\"\n","\n","\n","\n","FAVICON = './icon.svg'\n","\n","LLM_LIST = [\"gpt-3.5-turbo\", \"text-davinci-003\"]\n","\n","\n","DOC_1 = 'LLM'\n","DOC_2 = 'HTTP2'\n","\n","DOC_SUPPORTED = [DOC_1]\n","DOC_DEFAULT   = [DOC_1]\n","DOC_LABEL = \"Reference Docs\"\n","\n","\n","MODEL_WARNING = f\"Please paste your **{OPENAI_API_LINK}** and then **{KEY_INIT}**\"\n","\n","DOCS_WARNING = f\"\"\"Database Unloaded\n","Please check your **{TAB_3}** config and then **{KEY_INIT}**\n","Or you could uncheck **{DOC_LABEL}** to ask LLM directly\"\"\"\n","\n","\n","webui_title = \"\"\"\n","# OpenAI Chatbot Based on Vector Database\n","\"\"\"\n","\n","dup_link = f'''<a href=\"https://huggingface.co/spaces/ShawnAI/3GPP-ChatBot?duplicate=true\"\n","style=\"display:grid; width: 200px;\">\n","<img src=\"{get_logo(FORK_BADGE, \"addthis\")}\"></a>'''\n","\n","init_message = f\"\"\"This demonstration website is based on \\\n","**{OPENAI_LINK}** with **{LANGCHAIN_LINK}** and **{PINECONE_LINK}**\n","    1. Insert your **{OPENAI_API_LINK}** and click  `{KEY_INIT}`\n","    2. Insert your **Question** and click  `{KEY_SUBMIT}`\n","\"\"\"\n","\n","\n","#----------------------------------------------------------------------------------------------------------\n","#----------------------------------------------------------------------------------------------------------\n","\n","def init_model(api_key, emb_name, db_api_key, db_env, db_index):\n","    try:\n","        if not (api_key and api_key.startswith(\"sk-\") and len(api_key) > 50):\n","            return None,MODEL_NULL+DOCS_NULL,None,None,None,None\n","        \n","\n","\n","        llm_dict = {}\n","        for llm_name in LLM_LIST:\n","            if llm_name == \"gpt-3.5-turbo\":\n","                llm_dict[llm_name] = ChatOpenAI(model_name=llm_name,\n","                                                temperature = OPENAI_TEMP,\n","                                                openai_api_key = api_key)\n","            else:\n","                llm_dict[llm_name] = OpenAI(model_name=llm_name,\n","                                            temperature = OPENAI_TEMP,\n","                                            openai_api_key = api_key)\n","                    \n","        if not (emb_name and db_api_key and db_env and db_index):\n","            return api_key,MODEL_DONE+DOCS_NULL,llm_dict,None,None,None\n","            \n","        embeddings = HuggingFaceEmbeddings(model_name=emb_name)\n","\n","        pinecone.init(api_key     = db_api_key,\n","                      environment = db_env)\n","        db = Pinecone.from_existing_index(index_name = db_index,\n","                                          embedding  = embeddings)\n","\n","        return api_key, MODEL_DONE+DOCS_DONE, llm_dict, None, db, None\n","        \n","    except Exception as e:\n","        print(e)\n","        return None,MODEL_NULL+DOCS_NULL,None,None,None,None\n","\n","\n","def get_chat_history(inputs) -> str:\n","    res = []\n","    for human, ai in inputs:\n","        res.append(f\"Human: {human}\\nAI: {ai}\")\n","    return \"\\n\".join(res)\n","\n","def remove_duplicates(documents):\n","    seen_content = set()\n","    unique_documents = []\n","    for doc in documents:\n","        if doc.page_content not in seen_content:\n","            seen_content.add(doc.page_content)\n","            unique_documents.append(doc)\n","    return unique_documents\n","\n","def doc_similarity(query, db, top_k):\n","    docsearch = db.as_retriever(search_kwargs={'k':top_k})\n","    docs = docsearch.get_relevant_documents(query)\n","    udocs = remove_duplicates(docs)\n","    return udocs\n","\n","def user(user_message, history):\n","    return \"\", history+[[user_message, None]]\n","\n","def bot(box_message, ref_message,\n","        llm_dropdown, llm_dict, doc_list,\n","        db, top_k):\n","\n","    # bot_message = random.choice([\"Yes\", \"No\"])\n","    # 0 is user question, 1 is bot response\n","    question = box_message[-1][0]\n","    history  = box_message[:-1]\n","    \n","    if (not llm_dict):\n","        box_message[-1][1] = MODEL_WARNING\n","        return box_message, \"\", \"\"\n","\n","    if not ref_message:\n","        ref_message = question\n","        details = f\"Q:  {question}\"\n","    else:\n","        details = f\"Q:  {question}\\nR: {ref_message}\"\n","        \n","        \n","    llm = llm_dict[llm_dropdown]\n","    \n","    if DOC_1 in doc_list:\n","        if (not db):\n","            box_message[-1][1] = DOCS_WARNING\n","            return box_message, \"\", \"\"\n","        \n","        chain = load_qa_chain(llm, chain_type=\"stuff\")\n","        docs = doc_similarity(ref_message, db, top_k)\n","        delta_top_k = top_k - len(docs)\n","\n","        if delta_top_k > 0:\n","            docs = doc_similarity(ref_message, db, top_k+delta_top_k)\n","            \n","    else:\n","        chain = LLMChain(llm = llm,\n","                         prompt = PromptTemplate(template='{question}',\n","                                                input_variables=['question']),\n","                         output_key = 'output_text')\n","        docs = []\n","\n","    all_output = chain({\"input_documents\": docs,\n","                        \"question\": question,\n","                        \"chat_history\": get_chat_history(history)})\n","    \n","    bot_message = all_output['output_text']\n","\n","\n","    source = \"\".join([f\"\"\"<details> <summary>{doc.metadata[\"source\"]}</summary>\n","{doc.page_content}\n","</details>\"\"\" for i, doc in enumerate(docs)])\n","\n","    #print(source)\n","\n","    box_message[-1][1] = bot_message\n","    return box_message, \"\", [[details, bot_message + source]]\n","\n","#----------------------------------------------------------------------------------------------------------\n","#----------------------------------------------------------------------------------------------------------\n","\n","with gr.Blocks(\n","    title = TAB_1,\n","    theme = \"Base\",\n","    css = \"\"\".bigbox {\n","    min-height:250px;\n","}\n","\"\"\") as demo:\n","    llm = gr.State()\n","    chain_2 = gr.State() # not inuse\n","    vector_db = gr.State()\n","    gr.Markdown(webui_title)\n","    gr.Markdown(dup_link)\n","    gr.Markdown(init_message)\n","    \n","    with gr.Row():\n","        with gr.Column(scale=10):\n","            llm_api_textbox = gr.Textbox(\n","                label = \"OpenAI API Key\",\n","                # show_label = False,\n","                value = OPENAI_API_KEY,\n","                placeholder = \"Paste Your OpenAI API Key (sk-...) and Hit ENTER\",\n","                lines=1,\n","                type='password')\n","            \n","        with gr.Column(scale=1, min_width=BUTTON_MIN_WIDTH):\n","            \n","            init = gr.Button(KEY_INIT) #.style(full_width=False)\n","            model_statusbox = gr.HTML(MODEL_NULL+DOCS_NULL)\n","    \n","    with gr.Tab(TAB_1):\n","        with gr.Row():\n","            with gr.Column(scale=10):\n","                chatbot = gr.Chatbot(elem_classes=\"bigbox\")\n","            #with gr.Column(scale=1):\n","            with gr.Column(scale=1, min_width=BUTTON_MIN_WIDTH):\n","                doc_check = gr.CheckboxGroup(choices = DOC_SUPPORTED,\n","                                             value   = DOC_DEFAULT,\n","                                             label   = DOC_LABEL,\n","                                             interactive=True)\n","                llm_dropdown = gr.Dropdown(LLM_LIST,\n","                                           value=LLM_LIST[0],\n","                                           multiselect=False,\n","                                           interactive=True,\n","                                           label=\"LLM Selection\",\n","                                           )\n","        with gr.Row():\n","            with gr.Column(scale=10):\n","                query = gr.Textbox(label=\"Question:\",\n","                                   lines=2)\n","                ref = gr.Textbox(label=\"Reference(optional):\")\n","\n","            with gr.Column(scale=1, min_width=BUTTON_MIN_WIDTH):\n","\n","                clear = gr.Button(KEY_CLEAR)\n","                submit = gr.Button(KEY_SUBMIT,variant=\"primary\")\n","                \n","\n","    with gr.Tab(TAB_2):\n","        top_k = gr.Slider(1,\n","                          TOP_K_MAX,\n","                          value=TOP_K_DEFAULT,\n","                          step=1,\n","                          label=\"Vector similarity top_k\",\n","                          interactive=True)\n","        detail_panel = gr.Chatbot(label=\"Related Docs\")\n","    \n","    with gr.Tab(TAB_3):\n","        with gr.Row():\n","            emb_textbox = gr.Textbox(\n","                label = \"Embedding Model\",\n","                # show_label = False,\n","                value = EMBEDDING_MODEL,\n","                placeholder = \"Paste Your Embedding Model Repo on HuggingFace\",\n","                lines=1,\n","                interactive=True,\n","                type='email')\n","        with gr.Accordion(\"Pinecone Database for \"+DOC_1):\n","            with gr.Row():\n","                db_api_textbox = gr.Textbox(\n","                    label = \"Pinecone API Key\",\n","                    # show_label = False,\n","                    value = PINECONE_KEY,\n","                    placeholder = \"Paste Your Pinecone API Key (xx-xx-xx-xx-xx) and Hit ENTER\",\n","                    lines=1,\n","                    interactive=True,\n","                    type='password')\n","            with gr.Row():\n","                db_env_textbox = gr.Textbox(\n","                    label = \"Pinecone Environment\",\n","                    # show_label = False,\n","                    value = PINECONE_ENV,\n","                    placeholder = \"Paste Your Pinecone Environment (xx-xx-xx) and Hit ENTER\",\n","                    lines=1,\n","                    interactive=True,\n","                    type='email')\n","                db_index_textbox = gr.Textbox(\n","                    label = \"Pinecone Index\",\n","                    # show_label = False,\n","                    value = PINECONE_INDEX,\n","                    placeholder = \"Paste Your Pinecone Index (xxxx) and Hit ENTER\",\n","                    lines=1,\n","                    interactive=True,\n","                    type='email')\n","\n","    init_input  = [llm_api_textbox, emb_textbox, db_api_textbox, db_env_textbox, db_index_textbox]\n","    init_output = [llm_api_textbox, model_statusbox,\n","                   llm, chain_2,\n","                   vector_db, chatbot]\n","                \n","    llm_api_textbox.submit(init_model, init_input, init_output)\n","    init.click(init_model, init_input, init_output)\n","    \n","    submit.click(user,\n","                 [query, chatbot],\n","                 [query, chatbot],\n","                 queue=False).then(\n","        bot,\n","        [chatbot, ref,\n","         llm_dropdown, llm, doc_check,\n","         vector_db, top_k],\n","        [chatbot, ref, detail_panel]\n","    )\n","    \n","    clear.click(lambda: (None,None,None), None, [query, ref, chatbot], queue=False)\n","\n","#----------------------------------------------------------------------------------------------------------\n","#----------------------------------------------------------------------------------------------------------\n","    \n","if __name__ == \"__main__\":\n","    demo.launch(share        = True,\n","                inbrowser    = True,\n","                favicon_path = FAVICON)\n","\n"],"metadata":{"id":"cAAkB0iNcfon"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"/v2/external/notebooks/empty.ipynb","timestamp":1685377200316}],"gpuType":"T4","private_outputs":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}